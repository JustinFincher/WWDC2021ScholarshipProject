//#-hidden-code

//#-end-hidden-code

//: # 🧍 HumanScan: an interactive playground for scanning, rigging, and applying animations to real-life 3D human models

//: # 👀 Welcome
//: Welcome to **HumanScan**. Ever wondered if you can create a digital clone of yourself? Now it is possible with ARKit and Lidar-enabled devices. In this playground, we are gonna scan a point-cloud map of the environment you and your buddy is in, extract feature points around the tracked skeleton of your buddy, then make the scanned 3D human model animatable!
//:
//: # ‼️ Requirements
//: Please make sure that you are using a Lidar-equipped iPad Pro (2020 model).
//:
//: # ✳️ Steps
//: Follow the step-by-step guides (which is also displayed in the live view). Between switching steps, please make sure that you are standing still so your iPad can calibrate between AR sessions.
//: ## Step 1: Scan Human Point Clouds
//: ## Step 2: Capture Human Skeleton
//: ## Step 3: Match Skeleton and Point Clouds
//: ## Step 4: Generate Skinning Data
//: ## Step 5: Apply Animation
//:
//: # 🤖 Technical Explaination
//: ## What is a point cloud and why use it?
//: > A point cloud is a set of data points in space that represents 3D shapes. Point clouds are generated by 3D scanners (like the one you are currently holding) and they can be used for visualization, photogrammetry modeling, autonomous driving, etc.
//:
//: HumanScan samples depth maps and camera feed to derive vertex with color and position info from them. This provides better flexibility when compared with ARMeshAnchor approach where the ARMesh is generated with no color or UV info whatsoever.
//:
//: ## What is rigging and skinning?
//: > Rigging is the process of constructing a hierarchy of human joints, and Skinning is the process of binding the actual 3D mesh to the joint so the mesh can follow skeleton movement.
//:
//: These two processes are important to HumanScan as we are trying to convert a static scan into a animatable human model. This requires understanding of where the human is (to remove the background) and how point clouds are positioned around the human body (to create vertex-skeleton binding) so we can drive the skeleton to move the human scan later.
//:
//: ## What frameworks are used in this playground?
//: - MetalKit
//: - SceneKit
//:     - SCNGeometry
//:     - SCNSkinner
//: - SwiftUI
//: - Combine
//:
//: ## Is there room for further improvement?
//: Cloud points are still raw datasets and therefore lacks of smoothed detail required for a human model. I tried ARMeshAnchor with custom UV generation and camera feed ray-casting in hopes that I can un-project screen-space images into UV-space textures, but it failed due to inconsistent behaviour. (It is also worth noting Model I/O's `addUnwrappedTextureCoordinates` function is rather confusing with no documentation). Let's hope that in a year or two, there will be colored mesh scanning API for us to use.
//:
//: # References
//: - The [WWDC20 session 10611: Explore ARKit 4](https://developer.apple.com/videos/play/wwdc2020/10611/) sample project was used as a starter project for cloud-points accumulation.
//: - The SDF implementation for union/boolean operations are partially influenced by inigo quilez's [distance functions](https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm)
