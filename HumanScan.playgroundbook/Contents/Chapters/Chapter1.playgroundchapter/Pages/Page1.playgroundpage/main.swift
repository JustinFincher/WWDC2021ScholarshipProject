//#-hidden-code

//#-end-hidden-code

//: # üßç HumanScan: an interactive playground for scanning, rigging, and applying animations to real-life 3D human models

//: # üëÄ Welcome
//: Welcome to **HumanScan**. Ever wondered if you can create a digital clone of yourself? Now it is possible with ARKit and Lidar-enabled devices. In this playground, we are gonna scan a point-cloud map of the environment you and your buddy is in, extract feature points around the tracked skeleton of your buddy, then make the scanned 3D human model animatable!
//:
//: ![](Steps.png)
//:
//: # ‚ÄºÔ∏è Requirements
//: Please make sure that you are using a Lidar-equipped iPad Pro (2020 model).
//:
//: # ‚ú≥Ô∏è Steps
//: Follow the step-by-step guides (which is displayed in the live view). Between switching steps, please make sure that you are standing still so your iPad can calibrate between AR sessions.
//: ## Step 1: Scan Human Point Clouds
//: > Ask your friend to stand in a T-pose or a Â§ß-pose. The pose needs to be maintained throughout the process. Now walk around and use the iPad to scan your friends until the body are covered with enough point clouds.
//: ## Step 2: Capture Human Skeleton
//: > Now stand right before your friend and keep your friend's whole body within the viewport. Your iPad will display a skeleton based on your friends's T-pose / Â§ß-pose. Make sure it is matching the point cloud (you can still make small adjustments later).
//: ## Step 3: Match Skeleton and Point Clouds
//: > Your friend is now free to leave as we have captured enough information. But the skeleton might be a little bit off. Walk around and check if the skeleton is actually matched with the point cloud, and use sliders to move it until it is as fit as possible.
//: ## Step 4: Generate Skinning Data
//: > Now we need to wait for iPad to process the dataset. Press the button to remove background, auto-rigging and skinning the 3D model. After it is done you will be automatically moved the next step.
//: ## Step 5: Apply Animation
//: > Fun time! If you did a great job in scanning and adjusting skeleton positions, now is the time to see a fully rigged 3D model capable of playing custom animations! Press play to see your friend in different postures that are actually pre-recorded by myself. You now have a digital clone in the AR space, thnk about all silly things you can do with this technology!
//:
//: # ü§ñ Technical Explaination
//: ## What is a point cloud and why use it?
//: > A point cloud is a set of data points in space that represents 3D shapes. Point clouds are generated by 3D scanners (like the one you are currently holding) and they can be used for visualization, photogrammetry modeling, autonomous driving, etc.
//:
//: HumanScan samples depth maps and camera feed to derive vertex with color and position info from them. This provides better flexibility when compared with ARMeshAnchor approach where the ARMesh is generated with no color or UV info whatsoever.
//:
//: ## What is rigging and skinning?
//: > Rigging is the process of constructing a hierarchy of human joints, and Skinning is the process of binding the actual 3D mesh to the joint so the mesh can follow skeleton movement.
//:
//: These two processes are important to HumanScan as we are trying to convert a static scan into a animatable human model. This requires understanding of where the human is (to remove the background) and how point clouds are positioned around the human body (to create vertex-skeleton binding) so we can drive the skeleton to move the human scan later.
//:
//: ## What frameworks are used in this playground?
//: - MetalKit
//: - SceneKit
//:     - SCNGeometry
//:     - SCNSkinner
//: - SwiftUI
//: - Combine
//:
//: ## Is there room for further improvement?
//: Cloud points are still raw datasets and therefore lacks of smoothed detail required for a human model. I tried ARMeshAnchor with custom UV generation and camera feed ray-casting in hopes that I can un-project screen-space images into UV-space textures, but it failed due to inconsistent behaviour. (It is also worth noting Model I/O's `addUnwrappedTextureCoordinates` function is rather confusing with no documentation). Let's hope that in a year or two, there will be colored mesh scanning API for us to use.
//:
//: # ‚úçÔ∏è References
//: - The [WWDC20 session 10611: Explore ARKit 4](https://developer.apple.com/videos/play/wwdc2020/10611/) sample project was used as a starter project for cloud-points accumulation.
//: - The SDF implementation for union/boolean operations are partially influenced by inigo quilez's [distance functions](https://www.iquilezles.org/www/articles/distfunctions/distfunctions.htm) .
//:
//: # üë®‚Äçüíª Author
//: Haotian Zheng, a CMU MSMITE student ([Website](https://haotianzheng.com) ).
